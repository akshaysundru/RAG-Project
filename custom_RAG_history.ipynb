{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b6ac85",
   "metadata": {},
   "source": [
    "# Custom RAG Pipeline w/ History Implementation\n",
    "\n",
    "This program will be adding history to our RAG pipeline, modifying history from built in RAG history retrievers to creating custom chains with the \"|\" operator. Creating a custom retriever allows us better control over context injection, injecting in smaller chunks with metadata chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"llama3.2\"\n",
    "llm = OllamaLLM(model= MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36260e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(pdf_folder = \"./pdf_folder\"):\n",
    "    \n",
    "    document_loader = []\n",
    "\n",
    "    for root, dirs, files in os.walk(pdf_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".pdf\"):\n",
    "                full_path = os.path.join(root, file)\n",
    "                document_loader.append(full_path)\n",
    "\n",
    "    return document_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c51dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_loader = load_docs()\n",
    "document_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d0ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model.save(\"./local_models/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f1f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model =\"./local_models/all-MiniLM-L6-v2\" #embedding matrix model\n",
    "\n",
    "def embed_splitting(document_loader, embedding_model):\n",
    "    embeddings = HuggingFaceEmbeddings(model = embedding_model, encode_kwargs={'normalize_embeddings': True})\n",
    "\n",
    "    doc_store = []\n",
    "    for file_path in document_loader:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "\n",
    "        # Clean the metadata: keep only the filename, not full path\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = os.path.basename(file_path)\n",
    "\n",
    "        doc_store += docs\n",
    "\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size = 400,\n",
    "        chunk_overlap = 64\n",
    "        )\n",
    "    \n",
    "    #Make splits\n",
    "    splits = text_splitter.split_documents(doc_store)\n",
    "\n",
    "    return embeddings, splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b79a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "SPLITS_CACHE_PATH = \"splits_cache.pkl\"\n",
    "\n",
    "def get_splits(document_loader, embedding_model):\n",
    "    if os.path.exists(SPLITS_CACHE_PATH):\n",
    "        print(\"Loading cached splits from disk...\")\n",
    "        with open(SPLITS_CACHE_PATH, \"rb\") as f:\n",
    "            splits = pickle.load(f)\n",
    "        embeddings = HuggingFaceEmbeddings(model=embedding_model, encode_kwargs={'normalize_embeddings': True})\n",
    "    else:\n",
    "        print(\"Creating new splits...\")\n",
    "        embeddings, splits = embed_splitting(document_loader, embedding_model)\n",
    "        with open(SPLITS_CACHE_PATH, \"wb\") as f:\n",
    "            pickle.dump(splits, f)\n",
    "    return embeddings, splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd9095",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, splits = get_splits(document_loader, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc8a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_split = splits[106]\n",
    "example_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a021f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = example_split.metadata\n",
    "for key in metadata:\n",
    "    print(f\"{key}: {metadata[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b21538",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df2dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6e45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea441e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(embeddings.embed_query(\"test sentence\"))\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "if os.path.exists(\"faiss_index\"):\n",
    "    print(\"Loading FAISS index from disk...\")\n",
    "    vector_store = FAISS.load_local(\"faiss_index\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    print(\"Building FAISS index from scratch...\")\n",
    "    vector_store = FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "    )\n",
    "    vector_store.add_documents(splits)\n",
    "    vector_store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896f7f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the retriever object once\n",
    "semantic_retriever = vector_store.as_retriever(search_kwargs={'k': 4})\n",
    "\n",
    "# define your function to query it\n",
    "def semantic_search(retriever_obj, input_context: str):\n",
    "    return retriever_obj.invoke(input_context)\n",
    "\n",
    "# call the function with retriever and query string\n",
    "results = semantic_search(semantic_retriever, \"Explain transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ae1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results)):\n",
    "    source_data = results[i].metadata[\"source\"]\n",
    "    page = results[i].metadata[\"page\"]\n",
    "    page_content = results[i].page_content\n",
    "\n",
    "    print(f\"This is chunk number {i+1}.\\n\\n The source is {source_data}, found on page number {page}. \\n\\n The page content is {page_content} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7801f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "bm25_retriever.k = 4\n",
    "\n",
    "def keyword_search(retriever_obj, input_context: str):\n",
    "    return retriever_obj.invoke(input_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8978cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_results = keyword_search(bm25_retriever, \"Explain transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06891a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(keyword_results)):\n",
    "    source_data = keyword_results[i].metadata[\"source\"]\n",
    "    page = keyword_results[i].metadata[\"page\"]\n",
    "    page_content = keyword_results[i].page_content\n",
    "\n",
    "    print(f\"This is chunk number {i+1}.\\n\\n The source is {source_data}, found on page number {page}. \\n\\n The page content is {page_content} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0611271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(retrievers= [semantic_retriever, bm25_retriever], weights = [0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d2a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results = ensemble_retriever.invoke(\"Explain transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d923f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2f6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in combined_results:\n",
    "\n",
    "    print(i.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24772fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "result_list = [combined_results[0]]\n",
    "seen_pages = {combined_results[0].metadata[\"page_label\"]}\n",
    "\n",
    "while i < len(combined_results):\n",
    "    metadata = combined_results[i].metadata\n",
    "    page_label = metadata[\"page_label\"]\n",
    "\n",
    "    if page_label in seen_pages:\n",
    "        i += 1  # You MUST increment i here\n",
    "        continue\n",
    "\n",
    "    result_list.append(combined_results[i])\n",
    "    seen_pages.add(page_label)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d73bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fbbb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_template = \"\"\"You are an expert assistant answering based only on the provided context.\n",
    "\n",
    "The retrieved documents have been joined together and are separated by \"Chunk_n\", where n is the chunk number. Here is the context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use **ALL** relevant information above to answer the question below. If the answer isn't found in the chunks, say:\n",
    "\"I cannot answer this question because the necessary information was not found in the provided documents.\"\n",
    "\n",
    "❗Do not cite or mention any source files or page numbers in the body of your answer.\n",
    "\n",
    "At the end of your answer, add a single line in this format:\n",
    "\n",
    "Information was pulled from: <source_file_1>: pages <comma-separated page numbers>; <source_file_2>: pages <...>; ...\n",
    "\n",
    "Use only one entry per document, listing all unique page numbers where information was pulled from.\n",
    "Do not mention metadata_n, chunk_n, or include references in the main answer.\n",
    "\n",
    "Metadata:\n",
    "{metadata}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9486440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "\n",
    "def content_parser(results):\n",
    "    context_string = \"\"\n",
    "    for i in range(len(results)):\n",
    "\n",
    "        context_string += f\"\\nChunk_{i+1}:\\n\\n{results[i].page_content}\\n\\n\\n\"\n",
    "\n",
    "    return context_string \n",
    "\n",
    "chunks = content_parser(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d7ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_runnable = RunnableLambda(content_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a69a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_parser(results):\n",
    "    metadata_files = {}\n",
    "\n",
    "    for doc in results:\n",
    "        source = doc.metadata[\"source\"]\n",
    "        page = doc.metadata[\"page_label\"]\n",
    "\n",
    "        if source in metadata_files:\n",
    "            if page not in metadata_files[source]:\n",
    "                metadata_files[source].append(page)\n",
    "        else:\n",
    "            metadata_files[source] = [page]\n",
    "\n",
    "    metadata_string = \"This file uses the following sources:\"\n",
    "\n",
    "    for key in metadata_files:\n",
    "        pages = \", \".join(str(i) for i in metadata_files[key])\n",
    "        metadata_string += f\"\\n\\n{key}, pages {pages}\"\n",
    "        \n",
    "\n",
    "    return metadata_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204f6c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_runnable = RunnableLambda(metadata_parser)\n",
    "print(metadata_runnable.invoke(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10700fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata_parser(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5aca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embed_splitting import load_docs, get_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c196066",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", input_template),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7995f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = \"\"\"\n",
    "You are a question reformulator in a retrieval-based QA system.\n",
    "\n",
    "Given the latest user question and the preceding chat history, your task is:\n",
    "\n",
    "1. If the question is fully self-contained — i.e., it is grammatically and semantically complete and understandable on its own — return it **exactly as-is**.\n",
    "\n",
    "2. If the question is ambiguous without the chat history or depends on previous turns, rewrite it into a fully standalone, self-contained question.\n",
    "\n",
    "⚠️ Do NOT answer the question.  \n",
    "⚠️ Do NOT add any preamble, commentary, or extra explanation.  \n",
    "⚠️ Output **only** the final question text (either original or reformulated).\n",
    "\n",
    "Your job is to produce a single, context-independent question if needed — nothing else.\n",
    "\"\"\"\n",
    "\n",
    "contextual_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer1 = chain.invoke(\n",
    "    {'context': chunks,\n",
    "     'metadata': metadata,\n",
    "     'chat_history': [],\n",
    "     'input': \"Explain transformers\",\n",
    "     'question': \"Explain transformers\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6368e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = RunnableParallel({'context': chunk_runnable, 'metadata': metadata_runnable})\n",
    "b = a.invoke(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a4020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d2572",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ({\n",
    "    **b,\n",
    "    'chat_history': [],\n",
    "    'input': 'Explain transformers',\n",
    "    'question': 'Explain transformers'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444d77a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0bf884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = ensemble_retriever.invoke(\"Explain transformers\")\n",
    "\n",
    "metadata_dict = {}\n",
    "\n",
    "for i in range(len(answers)):\n",
    "    metadata_dict[f\"metadata {i}\"] = answers[i].metadata\n",
    "\n",
    "metadata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_aware_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cde28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_session_history('abb73283').messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f4be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rephrase_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7519c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def temp_pipeline():\n",
    "\n",
    "    session_id = str(uuid.uuid4())[:8]\n",
    "    print(f\"Session ID: {session_id}\")\n",
    "    \n",
    "    history = get_session_history(session_id)\n",
    "\n",
    "    print(f\"\\nModel {MODEL_NAME} has been initiated with memory. Please feel free to ask questions or type 'exit' to quit.\")\n",
    "    while True:\n",
    "        \n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Session ended. Have a good day.\")\n",
    "            break\n",
    "\n",
    "        print(f\"{user_input}\\n\\n\\n\")\n",
    "        \n",
    "        recontextual_chain = contextual_prompt | llm\n",
    "        rephrased_question = recontextual_chain.invoke(\n",
    "            {'chat_history': history.messages,\n",
    "             'input': user_input})\n",
    "        \n",
    "        print(f\"{rephrased_question} \\n\\n\\n\")\n",
    "\n",
    "        context_injection = (ensemble_retriever | RunnableParallel({'context': chunk_runnable, 'metadata': metadata_runnable})).invoke(rephrased_question)\n",
    "\n",
    "        print(\"Metadata:\\n\", context_injection['metadata'])\n",
    "        \n",
    "        response = history_aware_chain.invoke(\n",
    "            {**context_injection,\n",
    "            'input': user_input,\n",
    "            'question': rephrased_question},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        \n",
    "        print(f\"LLM: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e810af",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627510b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "history1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1811a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pipeline():\n",
    "\n",
    "    session_id = str(uuid.uuid4())[:8]\n",
    "    print(f\"Session ID: {session_id}\")\n",
    "    \n",
    "    history = get_session_history(session_id)\n",
    "\n",
    "    print(f\"\\nModel {MODEL_NAME} has been initiated with memory. Please feel free to ask questions or type 'exit' to quit.\")\n",
    "    while True:\n",
    "        \n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Session ended. Have a good day.\")\n",
    "            break\n",
    "\n",
    "        print(f\"{user_input}\\n\\n\\n\")\n",
    "        \n",
    "        MAX_HISTORY_TURNS = 1\n",
    "        recontextual_chain = contextual_prompt | llm\n",
    "        rephrased_question = recontextual_chain.invoke(\n",
    "            {'chat_history': history.messages[-MAX_HISTORY_TURNS:],\n",
    "             'input': user_input})\n",
    "        \n",
    "        print(f\"{rephrased_question} \\n\\n\\n\")\n",
    "\n",
    "        context_injection = (ensemble_retriever | RunnableParallel({'context': chunk_runnable, 'metadata': metadata_runnable})).invoke(rephrased_question)\n",
    "\n",
    "        expected_context = ensemble_retriever.invoke(user_input)\n",
    "        rephrased_context = ensemble_retriever.invoke(rephrased_question)\n",
    "\n",
    "        for i in expected_context:\n",
    "            source = i.metadata[\"source\"]\n",
    "            page_label = i.metadata[\"page_label\"]\n",
    "            print(f\"Expected metdata is:\\n\\n {source}, page number {page_label}\")\n",
    "\n",
    "        for i in rephrased_context:\n",
    "            source = i.metadata[\"source\"]\n",
    "            page_label = i.metadata[\"page_label\"]\n",
    "            print(f\"Rephrased question metdata is:\\n\\n {source}, page number {page_label}\")\n",
    "        \n",
    "        print(f\"Metadata:\\n, {context_injection['metadata']}\\n\\n\")\n",
    "        \n",
    "        response = history_aware_chain.invoke(\n",
    "            {**context_injection,\n",
    "            'input': user_input,\n",
    "            'question': rephrased_question},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        \n",
    "        print(f\"LLM: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b5fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab94273",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_session_history('37b5e65a').messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fd0f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "\n",
    "document = Document()\n",
    "\n",
    "document.add_heading('Document Title', 0)\n",
    "\n",
    "p = document.add_paragraph('A plain paragraph having some ')\n",
    "p.add_run('bold').bold = True\n",
    "p.add_run(' and some ')\n",
    "p.add_run('italic.').italic = True\n",
    "\n",
    "document.add_heading('Heading, level 1', level=1)\n",
    "document.add_paragraph('Intense quote', style='Intense Quote')\n",
    "\n",
    "document.add_paragraph(\n",
    "    'first item in unordered list', style='List Bullet'\n",
    ")\n",
    "document.add_paragraph(\n",
    "    'first item in ordered list', style='List Number'\n",
    ")\n",
    "\n",
    "\n",
    "records = (\n",
    "    (3, '101', 'Spam'),\n",
    "    (7, '422', 'Eggs'),\n",
    "    (4, '631', 'Spam, spam, eggs, and spam')\n",
    ")\n",
    "\n",
    "table = document.add_table(rows=1, cols=3)\n",
    "hdr_cells = table.rows[0].cells\n",
    "hdr_cells[0].text = 'Qty'\n",
    "hdr_cells[1].text = 'Id'\n",
    "hdr_cells[2].text = 'Desc'\n",
    "for qty, id, desc in records:\n",
    "    row_cells = table.add_row().cells\n",
    "    row_cells[0].text = str(qty)\n",
    "    row_cells[1].text = id\n",
    "    row_cells[2].text = desc\n",
    "\n",
    "document.add_page_break()\n",
    "\n",
    "document.save('demo.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a5093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import chunk_runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ce3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('session_store.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef132a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from session_history import get_session_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b585fe5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db148f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d1723d2a', 'a0bfaac3']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from session_history import get_session_history, list_sessions, load_session_messages, store\n",
    "\n",
    "list_sessions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d66cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76801aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Explain transformers\n",
      "AI: Transformers are electrical devices that transfer energy between two or more circuits through electromagnetic induction. They consist of two or more coils of wire, known as windings, which are wrapped around a common magnetic core.\n",
      "\n",
      "The basic principle of a transformer is based on the concept of electromagnetic induction, where a changing magnetic field induces an electric field in the surrounding conductors. In a transformer, the primary coil (connected to the power source) and secondary coil (connected to the load) are wound around a common magnetic core.\n",
      "\n",
      "When an alternating current flows through the primary coil, it generates a changing magnetic field, which induces an electromotive force (EMF) in the secondary coil. The direction of the induced EMF is such that it opposes the change in the magnetic field, creating a net flux that drives an electric current in the secondary coil.\n",
      "\n",
      "The ratio of the voltage and current between the primary and secondary coils determines the transformer's efficiency and performance. In a step-down transformer, the turns ratio (number of turns in the primary coil divided by the number of turns in the secondary coil) is typically greater than 1, reducing the voltage and increasing the current.\n",
      "\n",
      "Transformers are used to:\n",
      "\n",
      "* Isolate electrical circuits from each other\n",
      "* Change voltage levels (step-up or step-down)\n",
      "* Improve power transmission efficiency\n",
      "\n",
      "There are different types of transformers, including shell-type, toroidal, and core-type. Shell-type transformers have a central limb with two outer limbs for flux return, while toroidal transformers use a single, ring-shaped core. Core-type transformers use a magnetic core surrounded by coils.\n",
      "\n",
      "In addition to their primary function, transformers are also used in specialized applications such as position control systems, where they can be used to drive motors and provide precise control over the motor's rotation.\n",
      "\n",
      "Information was pulled from: ENSC3016_Course_Notes_Part_1_Electromagnetism_Transformers.pdf: pages 52, 51;\n",
      "Human: How are they utilised in AC machines?\n",
      "AI: Transformers are used in alternating current (AC) machines to transfer energy between the primary and secondary coils through electromagnetic induction. The primary coil is connected to the power source, and the secondary coil is connected to the load. When an alternating current flows through the primary coil, it generates a changing magnetic field that induces an electromotive force (EMF) in the secondary coil.\n",
      "\n",
      "The ratio of the voltage and current between the primary and secondary coils determines the transformer's efficiency and performance. In a step-down transformer, the turns ratio is typically greater than 1, reducing the voltage and increasing the current.\n",
      "\n",
      "Transformers are used to:\n",
      "\n",
      "* Isolate electrical circuits from each other\n",
      "* Change voltage levels (step-up or step-down)\n",
      "* Improve power transmission efficiency\n",
      "\n",
      "They are also used in specialized applications such as position control systems, where they can be used to drive motors and provide precise control over the motor's rotation.\n",
      "\n",
      "Information was pulled from: ENSC3016_Course_Notes_Part_1_Electromagnetism_Transformers.pdf: pages 52, 51;\n",
      "Human: How do AC machines differ in operation to DC machines?\n",
      "AI: Transformers can transfer energy between different types of electrical systems, including alternating current (AC) and direct current (DC). However, transformers are typically used to transfer energy between AC circuits.\n",
      "\n",
      "In the provided documents, it is mentioned that transformers are used in electrical machines systems, including AC machines. The documents also discuss the operation and characteristics of AC machines, such as torque and speed.\n",
      "\n",
      "Transformers can be used to change voltage levels in AC machines, which is useful for improving power transmission efficiency. They can also be used to isolate electrical circuits from each other, which is useful for safety purposes.\n",
      "\n",
      "In contrast, DC machines do not typically use transformers. Instead, they use a different type of electrical machine design that is suited for direct current applications.\n",
      "\n",
      "Information was pulled from: ENSC3016_Course_Notes_Part_2_Electric_Machines.pdf: page 4;\n",
      "Human: Go into further depth about three phase systems, are they used more in AC or DC machines?\n",
      "AI: Three-phase systems have several advantages over single-phase systems, including delivering more power and having constant delivered power at all times. This is due to the symmetrical set of sinusoidal currents with relative phase displacements of 120° that can be defined as follows:\n",
      "\n",
      ")120cos()240cos()120cos()cos(  tItIi tIi tIi\n",
      "\n",
      "In contrast, single-phase systems have pulsing power and are not as efficient.\n",
      "\n",
      "Three-phase systems are more commonly used in alternating current (AC) machines than direct current (DC) machines. This is because AC machines can utilize the benefits of three-phase systems to improve efficiency and performance.\n",
      "\n",
      "The documents do not specifically mention the use of three-phase systems in DC machines, but it is worth noting that DC machines typically do not require three-phase systems due to their design for direct current applications.\n",
      "\n",
      "On the other hand, many electrical generation and transmission systems are in the form of three-phase AC systems, which have several advantages over single-phase systems. These include delivering more power compared to a single-phase machine with the same amount of metal used in machine and lines.\n",
      "\n",
      "Information was pulled from: ENSC3016_Course_Notes_Part_1_Electromagnetism_Transformers.pdf: pages 51;\n",
      "Human: How do three phase systems deliver more power than single phase systems?\n",
      "AI: According to Chunk 4, three-phase systems deliver more power than single-phase systems because they have a symmetrical set of sinusoidal currents with relative phase displacements of 120°. This results in a constant delivered power at all times, which is not the case for single-phase systems.\n",
      "\n",
      "In a three-phase system, the total active power is given by:\n",
      "\n",
      "P = 3 * V * I * cos(θ)\n",
      "\n",
      "where P is the total active power, V is the line voltage, I is the line current, and θ is the phase angle. This expression shows that the total active power is directly proportional to the square of the line voltage and the line current.\n",
      "\n",
      "In contrast, single-phase systems have pulsing power and do not deliver as much power compared to a single-phase machine with the same amount of metal used in machine and lines.\n",
      "\n",
      "Information was pulled from: ENSC3016_Course_Notes_Part_1_Electromagnetism_Transformers.pdf: pages 51, 52;\n"
     ]
    }
   ],
   "source": [
    "a = load_session_messages('a0bfaac3')\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

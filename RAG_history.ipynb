{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dda8f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "import numpy as np\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be611b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat_history_db import ChatHistory\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a2030e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"llama3.2\"\n",
    "llm = OllamaLLM(model= MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "319f942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs():\n",
    "    \n",
    "    document_loader = []\n",
    "\n",
    "    for root, dirs, files in os.walk(\".\"):\n",
    "        # Skip chroma_db folder\n",
    "        if \"faiss\" in root or \"git\" in root:\n",
    "            continue\n",
    "        for file in files:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                document_loader.append(file)\n",
    "\n",
    "    return document_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0602f94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ENSC3016_Course_Notes_Part_1_Electromagnetism_Transformers.pdf',\n",
       " 'ENSC3016_Course_Notes_Part_2_Electric_Machines.pdf',\n",
       " 'Electric Machinery Fundamentals Textbook -- Chapman.pdf',\n",
       " 'ENSC3016 Study Guide 1-Review of Circuit Fundamentals.pdf',\n",
       " 'Three Phase Power System Fundamentals.pdf']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_loader = load_docs()\n",
    "document_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "faa47d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model =\"sentence-transformers/all-MiniLM-L6-v2\" #embedding matrix model\n",
    "\n",
    "def embed_splitting(document_loader, embedding_model):\n",
    "    embeddings = HuggingFaceEmbeddings(model = embedding_model, encode_kwargs={'normalize_embeddings': True})\n",
    "\n",
    "    doc_store = []\n",
    "    for file in document_loader:\n",
    "        loader = PyPDFLoader(file)\n",
    "        doc = loader.load()\n",
    "        doc_store += doc\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size = 400,\n",
    "        chunk_overlap = 64\n",
    "        )\n",
    "    \n",
    "    #Make splits\n",
    "    splits = text_splitter.split_documents(doc_store)\n",
    "\n",
    "    return embeddings, splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bab3bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, splits = embed_splitting(document_loader, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c62d3215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={'normalize_embeddings': True}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc527ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e552d420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS index from disk...\n"
     ]
    }
   ],
   "source": [
    "dim = len(embeddings.embed_query(\"test sentence\"))\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "if os.path.exists(\"faiss_index\"):\n",
    "    print(\"Loading FAISS index from disk...\")\n",
    "    vector_store = FAISS.load_local(\"faiss_index\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    print(\"Building FAISS index from scratch...\")\n",
    "    dim = len(embeddings.embed_query(\"test sentence\"))\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    vector_store = FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "    )\n",
    "    vector_store.add_documents(splits)\n",
    "    vector_store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e00f5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the retriever object once\n",
    "retriever = vector_store.as_retriever(search_kwargs={'k': 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6cc11b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x31a378310>, search_kwargs={'k': 4})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e76db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e623ec05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "| VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x31a378310>, search_kwargs={'k': 4}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x10ae0fec0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| OllamaLLM(model='llama3.2')\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x31a378310>, search_kwargs={'k': 4})), kwargs={}, config={'run_name': 'chat_retriever_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc7586a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an expert assistant answering based only on the provided context.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Use all relevant information above to answer the question below. If the answer isn't found in the chunks, say:\n",
    "    \"I cannot answer this question because the necessary information was not found in the provided documents.\"\n",
    "\n",
    "    When answering, cite the **source file name** and **slide/page number** if available.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "773c2cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f0557547",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6a0a440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Statefully manage chat history ###\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "305bc92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformers are electrical devices that transfer electrical energy between two circuits through electromagnetic induction. They consist of a coil of wire, known as the primary coil, and another coil, known as the secondary coil, which is connected to a load.\\n\\nIn the context of the provided documents, transformers are discussed in relation to their application in electrical machines and systems. The auto-wound transformer is mentioned as an example of a step-down transformer with a ratio of 2:1, where the current in each winding is equal to half of the load current.\\n\\nThe variable transformer, which has a single-layer coil wound on a toroidal core, allows for the smooth variation of output voltage by moving a carbon brush along the surface of the coil. This is an application of the auto-wound transformer principle.\\n\\nIn 3-phase systems, transformers can be used in sets of three single-phase transformers or as three-set windings on three limbs of a core, as shown in figures 6-10 and 6-11.\\n\\nThe construction of power transformers is also discussed. Small power transformers are often made with the shell-type construction, where coils are wound on the central limb of the core, and the two outer limbs provide symmetrical flux return paths (figure 6-3).\\n\\nIn general, transformers play a crucial role in electrical engineering for their ability to step-up or step-down voltages, making it possible to transmit energy efficiently over long distances.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"Explain transformers\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d4c3cc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformers have several key properties:\\n\\n1. **Magnetic Coupling**: Transformers use magnetic coupling to transfer energy between the primary and secondary coils. This is achieved by wrapping coils around a common magnetic core (Figure 6-6).\\n2. **Electromagnetic Induction**: The primary coil induces a voltage in the secondary coil through electromagnetic induction, which is a fundamental principle of electromagnetism.\\n3. **Ratio Control**: Transformers allow for ratio control, where the output voltage can be adjusted by changing the number of turns in the secondary coil relative to the primary coil.\\n4. **Step-Down and Step-Up**: Transformers are used to step down or step up voltages, making it possible to transmit energy efficiently over long distances.\\n5. **Toroidal Cores**: Toroidal transformers have a toroidal core, which is a ring-shaped core that allows for efficient magnetic coupling (Figure 6-5).\\n6. **Shell-Type Construction**: The shell-type construction features coils wound on the central limb of the core, with two outer limbs providing symmetrical flux return paths (Figure 6-3).\\n\\nThese properties make transformers essential components in electrical engineering, allowing for efficient transmission and transformation of energy.\\n\\nSource: \"Electromagnetism and Transformers\" by University of Western Australia'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"Explain their properties?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

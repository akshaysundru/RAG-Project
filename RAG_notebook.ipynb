{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ebaf35",
   "metadata": {},
   "source": [
    "# Basic RAG Pipeline Modularised\n",
    "\n",
    "This notebook contains a modularised version of the codecamp tutorial code, contained under one callable function that starts the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb2f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import numpy as np\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c01356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chathistory_db import ChatHistoryDB\n",
    "\n",
    "history_db = ChatHistoryDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1645cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"llama3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae6a67",
   "metadata": {},
   "source": [
    "I have created a function to start a model, this will be updated to include our vector store of embedded data when the model is started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f8c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs():\n",
    "    \n",
    "    document_loader = []\n",
    "\n",
    "    for root, dirs, files in os.walk(\".\"):\n",
    "        # Skip chroma_db folder\n",
    "        if \"faiss\" in root or \"git\" in root:\n",
    "            continue\n",
    "        for file in files:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                document_loader.append(file)\n",
    "\n",
    "    return document_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afcce31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ENSC3016_Course_Notes_Part_1_Electromagnetism_Transformers.pdf',\n",
       " 'Three Phase Power System Fundamentals.pdf',\n",
       " 'ENSC3016_Course_Notes_Part_2_Electric_Machines.pdf',\n",
       " 'Electric Machinery Fundamentals Textbook -- Chapman.pdf',\n",
       " 'ENSC3016 Study Guide 1-Review of Circuit Fundamentals.pdf']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_loader = load_docs()\n",
    "document_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f56235b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model =\"sentence-transformers/all-MiniLM-L6-v2\" #embedding matrix model\n",
    "\n",
    "def embed_splitting(document_loader, embedding_model):\n",
    "    embeddings = HuggingFaceEmbeddings(model = embedding_model, encode_kwargs={'normalize_embeddings': True})\n",
    "\n",
    "    doc_store = []\n",
    "    for file in document_loader:\n",
    "        loader = PyPDFLoader(file)\n",
    "        doc = loader.load()\n",
    "        doc_store += doc\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size = 400,\n",
    "        chunk_overlap = 64\n",
    "        )\n",
    "    \n",
    "    #Make splits\n",
    "    splits = text_splitter.split_documents(doc_store)\n",
    "\n",
    "    return embeddings, splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb6b167b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Akshay/miniconda3/envs/LLM_Train/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings, splits = embed_splitting(document_loader, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2932919d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={'normalize_embeddings': True}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7a4dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8fff133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(input):\n",
    "\n",
    "    input_vec = embeddings.embed_query(input)\n",
    "    texts = [doc.page_content for doc in splits]\n",
    "    vectors = embeddings.embed_documents(texts)\n",
    "\n",
    "    mag_ivec = np.linalg.norm(input_vec)\n",
    "\n",
    "    dot_product = []\n",
    "    mag_ovec = []\n",
    "\n",
    "    for context_vec in vectors:\n",
    "        dot_product.append(np.dot(input_vec, context_vec))\n",
    "        mag_ovec.append(np.linalg.norm(context_vec))\n",
    "\n",
    "    cos_sim = []\n",
    "    for i in range(len(mag_ovec)):\n",
    "        mag_ovec[i] *= mag_ivec\n",
    "        cos_sim.append(dot_product[i]/mag_ovec[i])\n",
    "\n",
    "    cossim_sort = list(enumerate(cos_sim))\n",
    "    cossim_sort.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return cossim_sort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd797be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_cosine_similarity(input1, input2):\n",
    "\n",
    "    vec1 = embeddings.embed_query(input1)\n",
    "    vec2 = embeddings.embed_query(input2)\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    magnitude1 = np.linalg.norm(vec1)\n",
    "    magnitude2 = np.linalg.norm(vec2)\n",
    "\n",
    "    # Handle the case where one or both magnitudes are zero to avoid division by zero\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0.0  # Or handle as appropriate for your application\n",
    "\n",
    "    return dot_product / (magnitude1 * magnitude2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6b016a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3938152084688825"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_cosine_similarity(\"Please explain transformers\", \"How do AC machines work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "185c93ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(106, 0.5388659022778391), (104, 0.500932552907889), (108, 0.4869626486402548), (115, 0.4810365954142485), (144, 0.4785985428899976)]\n"
     ]
    }
   ],
   "source": [
    "cossim_sort = cosine_similarity(\"Explain transformers\")\n",
    "print(cossim_sort[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c79aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < 3:\n",
    "    print(f\"The number {i+1} document is the {cossim_sort[i][0]} chunk, and reads the following: \\n\\n{splits[cossim_sort[i][0]].page_content}\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76ee2d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS index from disk...\n"
     ]
    }
   ],
   "source": [
    "dim = len(embeddings.embed_query(\"test sentence\"))\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "if os.path.exists(\"faiss_index\"):\n",
    "    print(\"Loading FAISS index from disk...\")\n",
    "    vector_store = FAISS.load_local(\"faiss_index\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    print(\"Building FAISS index from scratch...\")\n",
    "    dim = len(embeddings.embed_query(\"test sentence\"))\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    vector_store = FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "    )\n",
    "    vector_store.add_documents(splits)\n",
    "    vector_store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dedad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the retriever object once\n",
    "semantic_retriever = vector_store.as_retriever(search_kwargs={'k': 4})\n",
    "\n",
    "# define your function to query it\n",
    "def semantic_search(retriever_obj, input_context: str):\n",
    "    return retriever_obj.invoke(input_context)\n",
    "\n",
    "# call the function with retriever and query string\n",
    "results = semantic_search(semantic_retriever, \"Explain transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9af291a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x312e6b310>, search_kwargs={'k': 4})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10049fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='065ed451-14c6-4500-8de0-d852bce2b40a', metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2019-07-27T15:04:48+08:00', 'author': 'Ali Kharrazi', 'moddate': '2019-07-27T15:04:48+08:00', 'source': 'ENSC3016_Course_Notes_Part_1_Electromagnetism_Transformers.pdf', 'total_pages': 76, 'page': 51, 'page_label': '52'}, page_content='Transformer 52 \\n \\n \\n \\n   Figure 6-3 Shell-type transformers. \\n \\n \\n \\nFigure 6-4 Flux plot: shell-type transformer \\n \\n \\nToroidal transformers exploit the remarkable properties of toroidal coils described in section 3.6. \\nAlthough they are more expensive than shell-type transformers, the performance is better. They are used \\nin high -quality electronic equipment and for instrument transformers (see section 6.3) where \\nmeasurement accuracy is important. Typical toroidal transformers are shown in figure 6-5. \\n \\nFigure 6-5 Toroidal transformers.\\uf020\\n \\n \\n \\n6.2 Transformer Principle: \\nThe action of a transformer is most easily understood if the two coils are wound on opposite sides of a \\nmagnetic core, as shown in the model of figure 6 -6. This form is used for some low -cost transformers, \\nbut the magnetic coupling is not as good as with the shell-type construction. \\n \\n \\nFigure 6-6  Core-type transformer \\n \\n \\n \\nFigure 6 -7 is a schematic representation of the transformer. It will be assumed that the coupling is \\nperfect: the same magnetic flux \\uf066 passes through each turn of each coil. The coil connected to the source \\nis termed the primary, and the coil connected to the load is termed the secondary. It is usual to refer to \\nthe coils as windings.'),\n",
       " Document(id='7af74421-2394-4f47-a3a7-a7b5d62dd448', metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2019-07-27T15:04:48+08:00', 'author': 'Ali Kharrazi', 'moddate': '2019-07-27T15:04:48+08:00', 'source': 'ENSC3016_Course_Notes_Part_1_Electromagnetism_Transformers.pdf', 'total_pages': 76, 'page': 50, 'page_label': '51'}, page_content='51 Electrical Machines and Systems                                                                                                            \\n \\n6 Transformer \\n6.1 Introduction: \\nA transformer is a practical application of magnetically coupled coils. Usually the purpose is to transfer \\nenergy from one coil to the other, as in figure 6-1 where energy is transferred from the AC source to the \\nlamp through the space between the coils. \\n \\nFigure 6-1 Transformer principle \\nTransformers are essentially AC devices, because there has to be a change of flux to give an induced \\nvoltage in a coil. There are two main reasons for transferring energy in this way: \\n\\uf0b7 To provide electrical isolation between the source and the load. \\n\\uf0b7 To change the voltage and current levels. \\nThe coils are usually placed on a common magnetic core to improve the coupling. Figure 6-2 shows the \\nflux plots for two coupled coils (a) without a core, (b) with an open core, (c) with a closed core \\n \\nFigure 6-2 Effect of a magnetic core: \\n6.1.2 Practical aspects: \\nThe cores for high -frequency transformers are often made from magnetically soft ferrites, which are \\nelectrical insulators. For power frequencies, the cores are made from an iron alloy such as silicon steel.'),\n",
       " Document(id='037a3f33-3b03-4ec2-9a14-4bc18959a378', metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2019-07-27T15:04:48+08:00', 'author': 'Ali Kharrazi', 'moddate': '2019-07-27T15:04:48+08:00', 'source': 'ENSC3016_Course_Notes_Part_1_Electromagnetism_Transformers.pdf', 'total_pages': 76, 'page': 52, 'page_label': '53'}, page_content='𝑑𝜙\\n𝑑𝑡        (6 − 3) \\n𝑣2 = 𝑅2𝑖2 + 𝑁2\\n𝑑𝜙\\n𝑑𝑡                               (6 − 4) \\nThe sign difference arises from the reference directions for current in the two windings. \\nIf the resistances R1 and R2 are negligible, then equations 6-3 and 6-4 become: \\n𝑣1 ≈ 𝑁1\\n𝑑𝜙\\n𝑑𝑡          (6 − 5) \\n𝑣2 ≈ 𝑁2\\n𝑑𝜙\\n𝑑𝑡         (6 − 6) \\nDividing these equations gives the important result: \\n𝑣1\\n𝑣2\\n≈ 𝑁1\\n𝑁2\\n        (6 − 7) \\nThus, the secondary voltage can be made larger or smaller than the primary voltage by changing the \\nratio of the numbers of turns on the two windings. Voltage transformation is one of the most common \\nuses of transformers, on a large scale in electrical power transmission and distribution, and on a small \\nscale in the power supplies for electronic equipment. \\n6.2.2 Sinusoidal operation: \\nIf the voltage source is sinusoidal, then the core flux will also be sinusoidal, so we may put:'),\n",
       " Document(id='075b050a-e479-422d-ac4f-5d6f70982fcb', metadata={'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'creationdate': '2019-07-27T15:04:48+08:00', 'author': 'Ali Kharrazi', 'moddate': '2019-07-27T15:04:48+08:00', 'source': 'ENSC3016_Course_Notes_Part_1_Electromagnetism_Transformers.pdf', 'total_pages': 76, 'page': 55, 'page_label': '56'}, page_content='𝑽𝒔 = 𝑽𝟏 + 𝑽𝟐 = 𝟐𝑽𝟐 = 𝟐𝑽𝑳        (6 − 22) \\n𝑰𝑳 = 𝑰𝟏 + 𝑰𝟐 = 𝟐𝑰𝟏 = 𝟐𝑰𝑳            (6 − 23) \\nWhere VL is the voltage across the load and IS is the current supplied by the source. This auto-wound \\ntransformer behaves as a step-down transformer with a ratio of 2:1, and the current in each winding is \\nequal to half of the load current. \\nAn elegant application of the auto-wound transformer principle is the variable transformer, which has a \\nsingle-layer coil wound on a toroidal core. The output is taken from a carbon brush that makes contact \\nwith the surface of the coil; the brush can be moved smoothly from one end of the coil to the other, thus \\nvarying the output voltage. Examples of variable transformers are shown in figure 6-10. \\n \\n     \\n      Figure 6-10 Variable transformers. \\n \\n \\n \\n6.3.2 3 Phase Transformer: \\nIn 3 -phase systems, it is common practice to use sets of three single -phase transformers. It is also \\npossible, however, to make 3 -phase transformers with three sets of windings on three limbs of a core, \\nas shown in figure 6-11. \\n \\nFigure 6-11 3-phase transformer model')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde33313",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(results):\n",
    "    print(i+1, \"\\n\")\n",
    "    print(doc.page_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0ce75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "bm25_retriever.k = 4\n",
    "\n",
    "def bm25_keyword_search_lc(query):\n",
    "    return bm25_retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_results = bm25_keyword_search_lc(\"Explan transformers\")\n",
    "for i, doc in enumerate(keyword_results):\n",
    "    print(f\"Document {i+1}:\\n{doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e35c700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(retrievers= [semantic_retriever, bm25_retriever], weights = [0.67, 0.33], search_kwargs={\"k\": 3})\n",
    "\n",
    "def hybrid_search(retriever_obj, input_context: str):\n",
    "    return retriever_obj.invoke(input_context)\n",
    "\n",
    "hybrid_results = hybrid_search(ensemble_retriever, \"Explain transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6acfdbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hybrid_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e446ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(hybrid_results):\n",
    "    print(f\"Document {i+1}:\\n{doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ea09020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to create functions that create embeddings, load documents and split text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99e99b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_combined(model_name = MODEL_NAME):\n",
    "\n",
    "    llm = OllamaLLM(model = MODEL_NAME)\n",
    "\n",
    "    template = \"\"\"You are an expert assistant answering based only on the provided context.\n",
    "\n",
    "    Here are 3 relevant document chunks retrieved:\n",
    "\n",
    "    Chunk 1:\n",
    "    {chunk1}\n",
    "\n",
    "    Chunk 2:\n",
    "    {chunk2}\n",
    "\n",
    "    Chunk 3:\n",
    "    {chunk3}\n",
    "    \n",
    "    Chunk 4:\n",
    "    {chunk4}\n",
    "    \n",
    "    Use all relevant information above to answer the question below. If the answer isn't found in the chunks, say:\n",
    "    \"I cannot answer this question because the necessary information was not found in the provided documents.\"\n",
    "\n",
    "    When answering, cite the **source file name** and **slide/page number** if available.\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "    chain = prompt | llm\n",
    "    print(f\"\\n Model {model_name} has been initiated. Please feel free to ask any questions or type 'exit' to end this session\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You:\")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Have a good day.\")\n",
    "            break\n",
    "\n",
    "        context_docs = hybrid_search(ensemble_retriever, user_input)[:4]\n",
    "\n",
    "        # Pass context and question into the chain\n",
    "        chunks = [\n",
    "            f\"Source: {doc.metadata.get('source', 'unknown')}, Page: {doc.metadata.get('page', 'unknown')}\\n{doc.page_content}\"\n",
    "            for doc in context_docs\n",
    "        ]\n",
    "\n",
    "        response = chain.invoke({\n",
    "            \"chunk1\": chunks[0],\n",
    "            \"chunk2\": chunks[1],\n",
    "            \"chunk3\": chunks[2],\n",
    "            \"chunk4\": chunks[3],\n",
    "            \"question\": user_input\n",
    "        })\n",
    "\n",
    "        print(f\"LLM: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3abfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_combined()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
